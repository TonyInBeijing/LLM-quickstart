{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dda25d1-8955-4fb1-baa7-59bc7163320d",
   "metadata": {},
   "source": [
    "# åŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae656b6-04c0-415d-8341-2ed02ed28a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®é›† squad_v2\n",
    "from datasets import load_dataset\n",
    "dataset_v2 = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b5c14c-ce0d-4a97-8fd7-707bf92dca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ•°æ®é›†\n",
    "from datasets import ClassLabel,Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display,HTML\n",
    "\n",
    "def show_random_elements(dataset,num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"æ²¡æœ‰æ›´å¤šæ•°æ®\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0,len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0,len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column,typ in dataset.features.items():\n",
    "        if isinstance(typ,ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ,Sequence) and isinstance(typ.feature,ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    \n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "# show_random_elements(dataset_v2[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c858774e-1f00-43a6-9263-1d3a538a9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„å¤„ç†æ•°æ®\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab08a8b-1b86-45de-aaa1-673a441912d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fce75c-b13c-44df-9d44-f60773fe803a",
   "metadata": {},
   "source": [
    "## Tokenizer è¿›é˜¶æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94de616-43ff-4d1f-b7be-2500b16078f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # å®ä¾‹æœ€å¤§é•¿åº¦\n",
    "doc_stride = 128 # ä¸Šä¸‹æ–‡ä¿ç•™é•¿åº¦ï¼Œç”¨äºä¿è¯è¯­ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2adda8-7409-453f-a63a-cf96c98e254e",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca40adc3-d618-4823-8915-895f74bd5798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "pad_on_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff86683-c589-4451-ba9d-31d2811952f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´åˆä»¥ä¸Šæ­¥éª¤\n",
    "def prepare_train_features(examples):\n",
    "    # åˆ é™¤é—®é¢˜å·¦ä¾§ç©ºç™½å­—ç¬¦\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # å®šä¹‰ tokenizer\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # æ·»åŠ  start_positions å’Œ end_positions\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i] # è·å– input_ids []\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i) # è·å–æˆªå–åçš„æ¯ä¸€ä¸ª offsets å¯¹åº”çš„ sequence_ids()\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆå¼€å§‹å’Œç»“æŸçš„å­—ç¬¦ç´¢å¼•\n",
    "            start_char_index = answers[\"answer_start\"][0]\n",
    "            end_char_index = start_char_index + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰æ–‡æœ¬ä¸­å¼€å§‹tokenç´¢å¼•\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰æ–‡æœ¬ä¸­ç»“æŸtokenç´¢å¼•\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åœ¨ [token_start_index,token_end_index] ä¸­\n",
    "            if not (offsets[token_start_index][0] <= start_char_index and offsets[token_end_index][1] >= end_char_index):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å°† token_index ç§»åŠ¨åˆ°ç­”æ¡ˆä¸¤ç«¯çš„ token ä¸Š\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char_index:\n",
    "                    token_start_index += 1\n",
    "                while token_end_index > 1 and offsets[token_end_index][1] >= end_char_index:\n",
    "                    token_end_index -= 1\n",
    "                # è€ƒè™‘è¾¹ç•Œæƒ…å†µ\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1) \n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "# tokenized_datasets = dataset_v2.map(prepare_train_features,batched=True,remove_columns=dataset_v2[\"train\"].column_names)\n",
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b723ea30-df57-4c51-a4e4-bb99c13ae5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11873/11873 [00:02<00:00, 5167.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 131754\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 12134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä½œç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ä¸Š\n",
    "# print(dataset_v2[\"train\"].column_names)\n",
    "tokenized_datasets = dataset_v2.map(\n",
    "    prepare_train_features,\n",
    "    batched = True,\n",
    "    remove_columns = dataset_v2[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c144b7d-b2a4-44d1-bcf6-eac06b694360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 01:44:34.286501: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-06 01:44:34.316230: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-06 01:44:34.316262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-06 01:44:34.316865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-06 01:44:34.322035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 01:44:34.913163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# å¼•å…¥æ¨¡å‹\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# è®¾ç½®è¶…å‚æ•°\n",
    "batch_size = 64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "args = TrainingArguments(\n",
    "    output_dir = model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5, # ? \n",
    "    save_total_limit = 5,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01 # ?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad98fb3-ed16-4fc5-addf-d8be6dedcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ•´ç†\n",
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e85b84-5997-41e1-a0ce-8e2629271dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®ä¾‹åŒ–è®­ç»ƒå™¨\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e3fe9d-46bf-4f34-b133-d8a8801e0466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6177' max='6177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6177/6177 1:15:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.442000</td>\n",
       "      <td>1.293783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.166400</td>\n",
       "      <td>1.246210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.038400</td>\n",
       "      <td>1.321249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6177, training_loss=1.3532981681113532, metrics={'train_runtime': 4508.7095, 'train_samples_per_second': 87.666, 'train_steps_per_second': 1.37, 'total_flos': 3.873165421863629e+16, 'train_loss': 1.3532981681113532, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d3873c-d767-4f43-b1a2-91073243575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d210916-e290-47b7-bbc2-e210b951d6f5",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49eda6ff-145a-49f3-bf90-33993d8724f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "\n",
    "batch = {k: v.to(trainer.args.device) for k,v in batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output=trainer.model(**batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0e36539-270c-4968-bb44-8e6a13468682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape,output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe1459a-40cf-4ac6-95a8-1b2ede36f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 49,  37,  72,  80, 157,  17,  44,   0, 147, 202, 119,  52,  22,  31,\n",
       "           0, 120,   0,  94,  90,   0,   0,  56,  88, 152,  23,   0,   0, 124,\n",
       "           0,   0, 143,   0,  72,   0,   0,   0,  93,   0,  93, 105,  41,  13,\n",
       "          33,  55,  99,  29,  80,   0,  28,  29,   0,   0, 139,  70,  49,   0,\n",
       "          48,  62,  75,  19,  16,   0,   0,  12], device='cuda:0'),\n",
       " tensor([ 49,  40,  76,  81, 157,  19,  44,   0, 153, 204, 120,  52,  26,  33,\n",
       "           0, 121,   0,  97,  91,   0,   0,  56,  94, 153,  24,   0,   0, 142,\n",
       "          16,  52, 149,   0,  72,   0,   0,   0,  94,  27,  94, 128,  58,  15,\n",
       "          28,  56, 104,  30,  81, 143,  29,  30,   0,  80, 141,  71,  51,   0,\n",
       "          49,  63,  82,  20,  17,   0,   0,  15], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e3e524-3d64-40e8-8cf7-142288f172f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c4f65-1ff7-4809-8346-653d1d341940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144e80e7-0833-4127-bd07-f4a74ffe7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a97c8474-e8d4-4e30-9c5a-9bb19c3e1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11873/11873 [00:02<00:00, 5163.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_features = dataset_v2[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_v2[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25b9c02c-e57f-40ee-bc75-032e6d75d7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d98d241-1faf-444e-b16c-7cf8a6bbedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "778c52d1-58bf-4efb-a9f5-8d93c026eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7979fd1f-29b4-4a4c-8aa3-0f5aa6ba7595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 14.020666, 'text': 'France'},\n",
       " {'score': 9.298609, 'text': 'France.'},\n",
       " {'score': 8.040493, 'text': 'a region in France'},\n",
       " {'score': 7.4637156,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway'},\n",
       " {'score': 7.231961, 'text': 'in France'},\n",
       " {'score': 7.0140047,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark'},\n",
       " {'score': 6.4636974, 'text': 'region in France'},\n",
       " {'score': 6.157026,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland'},\n",
       " {'score': 5.6163383, 'text': 'Normandy, a region in France'},\n",
       " {'score': 3.859589,\n",
       "  'text': 'French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.67373,\n",
       "  'text': 'in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.6197875,\n",
       "  'text': '10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.3184361, 'text': 'a region in France.'},\n",
       " {'score': 2.9775162,\n",
       "  'text': 'Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 2.509904, 'text': 'in France.'},\n",
       " {'score': 1.7416402, 'text': 'region in France.'},\n",
       " {'score': 1.4835424,\n",
       "  'text': 'a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway'},\n",
       " {'score': 1.0651435, 'text': 'Denmark, Iceland and Norway'},\n",
       " {'score': 1.0338316,\n",
       "  'text': 'a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark'},\n",
       " {'score': 0.89428127, 'text': 'Normandy, a region in France.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = dataset_v2[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f81dd4-0650-45ed-8242-300f5b5dc332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['France', 'France', 'France', 'France'],\n",
       " 'answer_start': [159, 159, 159, 159]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_v2[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbc4121-98d6-4504-aac4-c1ae104d9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = dataset_v2[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1cbd9ce-fe6f-461b-a705-6d0e24c2fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "        predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8a27852-6c59-4bfe-8c23-0da2172f4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 11873 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 12134 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11873/11873 [00:14<00:00, 837.49it/s]\n"
     ]
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(dataset_v2[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41027f16-e2d9-4990-bdd5-dcc36ae2d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1449796/4244615236.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\")\n",
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for squad_v2 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad_v2/squad_v2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ffd419-2a1e-4cd6-a0cd-8f23f06346bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 61.265055167186055,\n",
       " 'f1': 64.92858615097008,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 64.00134952766531,\n",
       " 'HasAns_f1': 71.33891757261564,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 58.53658536585366,\n",
       " 'NoAns_f1': 58.53658536585366,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 61.29032258064516,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 64.92858615097029,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dataset_v2[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda3fdf-62d8-45ef-8e5c-811308bcdede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8dee74-768a-48ef-8bbf-14126a89efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
