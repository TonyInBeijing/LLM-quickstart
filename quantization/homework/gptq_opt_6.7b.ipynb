{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9099db-323a-424d-bf56-7d449a87bea2",
   "metadata": {},
   "source": [
    "# 使用 GPTQ 量化 OPT-6.7B 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41ead55-cf96-4ea7-8ae6-ab23dfbd5107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"facebook/opt-6.7b\"\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "    bits = 4,\n",
    "    group_size = 128,\n",
    "    dataset = \"wikitext2\",\n",
    "    desc_act = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f4ee82-3d17-40b7-8e44-7f263b2d2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 09:56:35.271009: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 09:56:35.302594: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 09:56:35.456533: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-19 09:56:35.456565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-19 09:56:35.486239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-19 09:56:35.553746: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 09:56:36.400773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|█████████████████████████████| 2/2 [00:13<00:00,  6.59s/it]\n",
      "Quantizing model.decoder.layers blocks :   0%|                      | 0/32 [00:00<?, ?it/s]\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:06<00:30,  6.19s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:11<00:21,  5.48s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:16<00:15,  5.30s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:21<00:10,  5.16s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:26<00:05,  5.15s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [00:46<00:00, 10.21s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :   3%|▍             | 1/32 [00:49<25:33, 49.46s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:26,  5.26s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:21,  5.26s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:21<00:10,  5.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:26<00:05,  5.30s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [02:38<00:00, 48.32s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :   6%|▋          | 2/32 [03:42<1:00:58, 121.94s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:26,  5.30s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.24s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.11s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:20<00:10,  5.04s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [02:49<00:00, 52.15s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :   9%|█          | 3/32 [06:45<1:12:30, 150.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.93s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.94s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.93s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:24<00:04,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [02:43<00:00, 50.49s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  12%|█▍         | 4/32 [09:44<1:15:24, 161.59s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.93s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.90s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:24<00:04,  4.97s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:15<00:00, 61.25s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  16%|█▋         | 5/32 [13:15<1:20:37, 179.15s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.91s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.91s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.91s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.90s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:24<00:04,  4.97s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [02:54<00:00, 54.14s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  19%|██         | 6/32 [16:25<1:19:13, 182.83s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.95s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.94s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:24<00:04,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:40<00:00, 69.78s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  22%|██▍        | 7/32 [20:19<1:23:15, 199.83s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.93s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.94s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.92s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.91s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:24<00:04,  4.97s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:03<00:00, 57.21s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  25%|██▊        | 8/32 [23:37<1:19:40, 199.17s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.11s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:19,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.96s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:11<00:00, 60.06s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  28%|███        | 9/32 [27:04<1:17:18, 201.67s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:14,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.06s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:24<00:00, 64.29s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  31%|███▏      | 10/32 [30:44<1:15:56, 207.10s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.03s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.06s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.04s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:20<00:10,  5.02s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:43<00:00, 70.76s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  34%|███▍      | 11/32 [34:45<1:16:08, 217.55s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:20<00:10,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:43<00:00, 70.69s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  38%|███▊      | 12/32 [38:48<1:15:06, 225.33s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.06s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.03s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:20<00:10,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:15<00:00, 61.27s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  41%|████      | 13/32 [42:19<1:09:58, 220.96s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:23<01:58, 23.61s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:47<01:34, 23.61s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:08<01:07, 22.39s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:29<00:43, 21.82s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:47<00:20, 20.62s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [05:45<00:00, 94.55s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  44%|████▍     | 14/32 [48:23<1:19:13, 264.09s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.97s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.05s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:39<00:00, 69.50s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  47%|████▋     | 15/32 [52:17<1:12:16, 255.11s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:26<02:11, 26.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:52<01:45, 26.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:16<01:15, 25.06s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:39<00:49, 24.50s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [02:00<00:23, 23.30s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [06:09<00:00, 99.94s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  50%|█████     | 16/32 [58:46<1:18:46, 295.41s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:08<00:42,  8.54s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:13<00:25,  6.44s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:18<00:17,  5.77s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:23<00:10,  5.46s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:28<00:05,  5.36s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:20<00:00, 61.93s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  53%|████▎   | 17/32 [1:02:22<1:07:51, 271.43s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:23<01:58, 23.62s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:47<01:34, 23.61s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:08<01:07, 22.39s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:29<00:43, 21.82s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:47<00:20, 20.62s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [05:52<00:00, 96.83s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  56%|████▌   | 18/32 [1:08:32<1:10:16, 301.20s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:24,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.98s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.06s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:48<00:00, 72.43s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  59%|████▊   | 19/32 [1:12:35<1:01:28, 283.71s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:26<02:11, 26.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:52<01:45, 26.25s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:16<01:15, 25.04s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:39<00:48, 24.48s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [02:00<00:23, 23.28s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:18<00:00, 102.94s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  62%|█████   | 20/32 [1:19:13<1:03:37, 318.09s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:10<00:50, 10.18s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:15<00:28,  7.13s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:20<00:18,  6.15s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:25<00:11,  5.70s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:30<00:05,  5.51s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:32<00:00, 65.47s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  66%|██████▌   | 21/32 [1:23:01<53:21, 291.08s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:23<01:58, 23.78s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:47<01:34, 23.69s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:09<01:08, 22.87s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:30<00:44, 22.12s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:48<00:20, 20.85s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:08<00:00, 102.15s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  69%|██████▉   | 22/32 [1:29:30<53:23, 320.31s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:04<00:24,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.05s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:55<00:00, 74.96s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  72%|███████▏  | 23/32 [1:33:43<45:02, 300.26s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:26<02:13, 26.60s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:53<01:46, 26.62s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:17<01:16, 25.39s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:41<00:49, 24.84s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [02:02<00:23, 23.57s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:39<00:00, 109.87s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  75%|███████▌  | 24/32 [1:40:46<44:54, 336.84s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.03s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:09<00:19,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.97s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.96s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.03s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:29<00:00, 66.11s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  78%|███████▊  | 25/32 [1:44:34<35:29, 304.16s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:24<02:02, 24.41s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:46<01:31, 22.79s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:07<01:06, 22.21s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:26<00:41, 20.85s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:45<00:20, 20.26s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:14<00:00, 104.71s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  81%|████████▏ | 26/32 [1:51:09<33:09, 331.52s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:09<00:47,  9.41s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:14<00:27,  6.83s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:19<00:17,  5.98s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:24<00:11,  5.59s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:29<00:05,  5.44s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [04:10<00:00, 78.85s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  84%|████████▍ | 27/32 [1:55:39<26:04, 312.97s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:27<02:17, 27.46s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:52<01:43, 25.84s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:16<01:15, 25.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:38<00:47, 23.95s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [02:00<00:23, 23.26s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:40<00:00, 110.48s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  88%|████████▊ | 28/32 [2:02:45<23:07, 346.95s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.07s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:20,  5.04s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:15<00:15,  5.02s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:20<00:10,  5.02s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.08s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [03:32<00:00, 67.01s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  91%|█████████ | 29/32 [2:06:37<15:37, 312.50s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:24<02:01, 24.32s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:46<01:31, 22.82s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:07<01:06, 22.26s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:26<00:41, 21.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:45<00:20, 20.32s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:17<00:00, 105.69s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  94%|█████████▍| 30/32 [2:13:17<11:17, 338.71s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:05<00:25,  5.01s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:10<00:19,  5.00s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [00:14<00:14,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [00:19<00:09,  4.99s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [00:25<00:05,  5.05s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|████████████████████| 6/6 [04:04<00:00, 77.99s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks :  97%|█████████▋| 31/32 [2:17:41<05:16, 316.44s/it]\u001b[A\n",
      "Quantizing layers inside the block:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Quantizing layers inside the block:  17%|███▎                | 1/6 [00:27<02:16, 27.27s/it]\u001b[A\n",
      "Quantizing layers inside the block:  33%|██████▋             | 2/6 [00:51<01:42, 25.67s/it]\u001b[A\n",
      "Quantizing layers inside the block:  50%|██████████          | 3/6 [01:16<01:15, 25.12s/it]\u001b[A\n",
      "Quantizing layers inside the block:  67%|█████████████▎      | 4/6 [01:38<00:47, 23.79s/it]\u001b[A\n",
      "Quantizing layers inside the block:  83%|████████████████▋   | 5/6 [01:59<00:23, 23.11s/it]\u001b[A\n",
      "Quantizing layers inside the block: 100%|███████████████████| 6/6 [06:37<00:00, 109.45s/it]\u001b[A\n",
      "Quantizing model.decoder.layers blocks : 100%|██████████| 32/32 [2:24:45<00:00, 271.43s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a01eba-a72e-435e-8594-a1b4a170a705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('qweight',\n",
       "               tensor([[-1533367701,  2022282410, -1182414405,  ..., -1537760937,\n",
       "                          968190105, -1218869078],\n",
       "                       [-1973762678, -1990888091,  1451669112,  ...,  1767327094,\n",
       "                         1485273242, -1769109111],\n",
       "                       [ -890521657,  1705355194,  2042256023,  ...,  1401453177,\n",
       "                         -963081656, -1212573545],\n",
       "                       ...,\n",
       "                       [-1192544404,   697191045,  1432856694,  ...,  1967820506,\n",
       "                        -1482119368, -1787262823],\n",
       "                       [-1736931943, -1753576812,  2027985786,  ..., -1757504693,\n",
       "                         2090308806, -1987483739],\n",
       "                       [ 1549191317,  1151064006, -1735993498,  ..., -1317428394,\n",
       "                        -1182375288, -1199925157]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('qzeros',\n",
       "               tensor([[2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       ...,\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('scales',\n",
       "               tensor([[0.0039, 0.0035, 0.0035,  ..., 0.0030, 0.0028, 0.0044],\n",
       "                       [0.0032, 0.0030, 0.0036,  ..., 0.0034, 0.0028, 0.0033],\n",
       "                       [0.0038, 0.0029, 0.0039,  ..., 0.0043, 0.0025, 0.0035],\n",
       "                       ...,\n",
       "                       [0.0033, 0.0034, 0.0033,  ..., 0.0032, 0.0038, 0.0030],\n",
       "                       [0.0030, 0.0028, 0.0027,  ..., 0.0028, 0.0038, 0.0026],\n",
       "                       [0.0034, 0.0035, 0.0034,  ..., 0.0027, 0.0027, 0.0042]],\n",
       "                      device='cuda:0', dtype=torch.float16)),\n",
       "              ('g_idx',\n",
       "               tensor([ 0,  0,  0,  ..., 31, 31, 31], device='cuda:0', dtype=torch.int32)),\n",
       "              ('bias',\n",
       "               tensor([ 0.0061,  0.0129, -0.0215,  ...,  0.0157, -0.0144,  0.0106],\n",
       "                      device='cuda:0', dtype=torch.float16))]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'infeatures': 4096,\n",
       " 'outfeatures': 4096,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'trainable': False,\n",
       " 'maxq': 15,\n",
       " 'device': device(type='cuda', index=0),\n",
       " '_use_act_order': False,\n",
       " 'width': 4096,\n",
       " 'q4': 94318223188032}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece9ff7d-6e44-4e2e-a5f6-350765bd8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.save_pretrained(\"../models/opt-6.7b-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764bfbd6-10b0-4b20-8e8e-26a1f5c2bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to see you're still around.\n",
      "I'm still around, just not as active as I used to be.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "text = \"Merry Christmas! I'm glad to\"\n",
    "\n",
    "input = tokenizer(text,return_tensors = \"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**input,max_new_tokens=1024)\n",
    "print(tokenizer.decode(out[0],skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e01bf-57fd-4836-bf31-c7b1d9251bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
