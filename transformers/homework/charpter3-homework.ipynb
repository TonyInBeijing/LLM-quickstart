{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26913168-b5be-4f74-abd5-b23720dc63ad",
   "metadata": {},
   "source": [
    "# HomeWork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccef861-a597-48b6-9837-23b3751ce00c",
   "metadata": {},
   "source": [
    "## 1.Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1e760d-45d8-4f5a-8549-f1a95450d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': 0.8935, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.915, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.9777, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'B-MISC', 'score': 0.9996, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'B-LOC', 'score': 0.9995, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9994, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-LOC', 'score': 0.9996, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "# dslim/bert-base-NER\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Hugging Face is a French company based in New York City.\"\n",
    "\n",
    "preds = nlp(example)\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e0662-2d1e-4753-bf2b-126629b33daa",
   "metadata": {},
   "source": [
    "## 2.Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d400ce-2390-4770-97fe-45c56b0edc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9296, start: 114, end: 123, answer:  Beijing.\n"
     ]
    }
   ],
   "source": [
    "# timpal0l/mdeberta-v3-base-squad2\n",
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\", \"timpal0l/mdeberta-v3-base-squad2\")\n",
    "question = \"What is the capital of China?\"\n",
    "context = \"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\"\n",
    "preds = qa_model(question = question, context = context)\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed7521-b953-4c49-824d-78cc7cac30d3",
   "metadata": {},
   "source": [
    "## 3.Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8757b3-cde2-4314-a7d7-2730908a1472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 230, but your input_length is only 128. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Transformer is the first sequence transduction model based entirely on attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n"
     ]
    }
   ],
   "source": [
    "# Falconsai/text_summarization\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=230, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23571c67-8013-48c2-bb12-f2df366da8f4",
   "metadata": {},
   "source": [
    "## 4.Audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df860ed1-794a-44a4-a219-df77fa0d95b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████| 1.83k/1.83k [00:00<00:00, 11.5MB/s]\n",
      "pytorch_model.bin: 100%|████████████████████████| 378M/378M [00:37<00:00, 10.0MB/s]\n",
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at abdelhalim/Shower_Sound_Recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at abdelhalim/Shower_Sound_Recognition and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "preprocessor_config.json: 100%|███████████████████| 215/215 [00:00<00:00, 1.64MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7651, 'label': 'no_shower'}, {'score': 0.2349, 'label': 'shower'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abdelhalim/Shower_Sound_Recognition\n",
    "from transformers import pipeline\n",
    "\n",
    "audio_input = 'data/audio/mlk.flac'\n",
    "classifier = pipeline(\"audio-classification\", model=\"abdelhalim/Shower_Sound_Recognition\")\n",
    "preds = classifier(audio_input)\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ed683-147c-4ec0-b872-4c1160e7a43b",
   "metadata": {},
   "source": [
    "## 5.Automatic speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f10152-5bf9-451f-92b6-941339621dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|████████████████████████████| 1.94k/1.94k [00:00<00:00, 9.86MB/s]\n",
      "model.safetensors: 100%|████████████████████████| 290M/290M [00:30<00:00, 9.64MB/s]\n",
      "generation_config.json: 100%|█████████████████| 1.53k/1.53k [00:00<00:00, 9.51MB/s]\n",
      "tokenizer_config.json: 100%|██████████████████████| 805/805 [00:00<00:00, 4.03MB/s]\n",
      "vocab.json: 100%|████████████████████████████████| 798k/798k [00:00<00:00, 805kB/s]\n",
      "tokenizer.json: 100%|█████████████████████████| 2.41M/2.41M [00:01<00:00, 2.20MB/s]\n",
      "merges.txt: 100%|████████████████████████████████| 456k/456k [00:00<00:00, 590kB/s]\n",
      "normalizer.json: 100%|█████████████████████████| 52.7k/52.7k [00:00<00:00, 218kB/s]\n",
      "added_tokens.json: 100%|███████████████████████| 34.6k/34.6k [00:00<00:00, 395kB/s]\n",
      "special_tokens_map.json: 100%|████████████████| 1.83k/1.83k [00:00<00:00, 2.65MB/s]\n",
      "preprocessor_config.json: 100%|██████████████████| 185k/185k [00:00<00:00, 277kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n"
     ]
    }
   ],
   "source": [
    "# openai/whisper-base.en\n",
    "from transformers import pipeline\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "text = transcriber(\"data/audio/mlk.flac\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaea5a-f47b-4798-8fe7-95cf3814b63a",
   "metadata": {},
   "source": [
    "## 6.Image Classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a125f59f-87d2-47b3-8f59-ec64b61f76f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|█████████████████████████████| 69.6k/69.6k [00:00<00:00, 250kB/s]\n",
      "pytorch_model.bin: 100%|████████████████████████| 114M/114M [00:11<00:00, 9.69MB/s]\n",
      "preprocessor_config.json: 100%|███████████████████| 266/266 [00:00<00:00, 1.82MB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7292, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0633, 'label': 'wombat'}\n",
      "{'score': 0.0248, 'label': 'badger'}\n",
      "{'score': 0.0147, 'label': 'marmot'}\n",
      "{'score': 0.0092, 'label': 'tabby, tabby cat'}\n"
     ]
    }
   ],
   "source": [
    "# facebook/convnext-tiny-224\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"image-classification\", model=\"facebook/convnext-tiny-224\")\n",
    "preds = classifier(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df275a-10c1-491f-9937-e3e712dd4d09",
   "metadata": {},
   "source": [
    "## 7.Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d186d0a-46c4-45b5-bbe7-d490b8869b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9865,\n",
       "  'label': 'cat',\n",
       "  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facebook/detr-resnet-101\n",
    "from transformers import pipeline\n",
    "detector = pipeline(task=\"object-detection\",model=\"facebook/detr-resnet-50\")\n",
    "preds = detector(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
